{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science has risen to prominence in the last decade due to its capabilities in predictive algorithms. While many business verticals value the benefits of predictive algorithms using Data Science, insurance companies place a lot of importance as data science and predictive algorithms helps them keeps premium low. Data is always been at the core of what insurance companies do analyzing data such as claims, what kind of a vehicle one drives, how many miles do they drive per day among other.\n",
    "<br><br>\n",
    "The data science field is gaining strength with improvements in technology, availability of statistical libraries to compute regression or classifications of data collected. Actuaries, the data scientists at insurance companies as they were called a decade ago, used to collate data from different sources and analyze the premium and claim data to identify fraudulent transactions that helped them keep the premiums low. If anything, data science technology of today has given far more tools to perform their analysis.\n",
    "<br><br>\n",
    "The data has a few ordinal, categorical data that needs to be parsed and categorized properly.\n",
    "<br><br>\n",
    "Our goal is to predict a binary outcome of 1, to indicate safe driver, or 0, to indicate that the drivers' data needs a review. We will also look at the continuous variables and fill in the missing data with the mean or median in order to not skew our results.\n",
    "<br><br>\n",
    "After cleaning up the data and filling in missing data we will look at the features and their correlation so that we can drop highly correlated data which may impact our results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd7379ed8711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Classifiers ensembling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages of Python that we will/may use in this notebook\n",
    "# pandas and numpy for dataframe creation and manipulation\n",
    "# matplot lib for data visualization\n",
    "# sklearn for statistical algorithms and splitting the dataset to training and testing datasets\n",
    "\n",
    "# General\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Features pre-processing and principal component analysis (pca)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "# Classifiers ensembling\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "# Classifiers evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Random resampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Tuning hyperparameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "\n",
    "# Other\n",
    "from time import time\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Ploting\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('white')\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "\n",
    "# Suppressing annoying harmless error\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# ROC-AUC Calculation modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the local drive\n",
    "\n",
    "safe_driver = pd.read_excel('IT_3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data has any negaitve values\n",
    "\n",
    "safe_driver.where(safe_driver < 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any NULL data that need to be dropped\n",
    "safe_driver.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_driver.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x, _ = stats.boxcox(safe_driver['credit_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(safe_driver.credit_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and see if we have an imbalanced class label in the dataset\n",
    "# Calculate the percentage of success data ('target' == 1) with respect to the failure data ('target' == 0)\n",
    "\n",
    "true_claims = (safe_driver['target'] == 1).sum()\n",
    "print('True Claims is  {}'.format(true_claims))\n",
    "\n",
    "total_records = len(safe_driver['target'])\n",
    "print('Total number of records is {}'.format(total_records))\n",
    "\n",
    "print('The percentage of true claims is {}%'.format(\n",
    "    round(true_claims / total_records * 100), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is indeed imbalanced. We will balance it later using SMOTE technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains several categorical data that ends with `_bucket` that need to be either dropped or converted to numerical values using dummies. All features that are of type object are categorical variables that needs to either:<br>\n",
    "<br>\n",
    "a. Converted to numeric using dummies<br>\n",
    "b. Dropped or<br>\n",
    "c. Assigned a binary value<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = safe_driver.select_dtypes(include=['object']).copy()\n",
    "print(cat_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the categorical variables we retain the following:<br>\n",
    "<br>\n",
    "1. Gender<br>\n",
    "2. Marital_Status<br>\n",
    "3. Vehicle_Type, and<br>\n",
    "4. Age_bucket<br>\n",
    "<br>\n",
    "EngineHP_bucket, Years_Experience_bucket, Miles_driven_annually_bucket, credit_history_bucket have a corresponding continuous variable. Creating each with their own dummies along with the continuous variable does not make sense. We will keep the Age_bucket as there is no continuous variable to represent age.<br>\n",
    "<br>\n",
    "We can split the dataset by State (one sub-dataset for each state) and analyze each state by itself. As each US state has its own regulations it may make sense to analyze each state by itself. We could aggregate our results across states later to get a national statistic.<br>\n",
    "<br>\n",
    "Or, for now, we could drop the State column and analyze the data across the nation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop these 5 columns: ID, EngineHP_bucket, Years_Experience_bucket, Miles_driven_annually_bucket, credit_history_bucket\n",
    "\n",
    "safe_driver.drop(['ID', 'EngineHP_bucket', 'Years_Experience_bucket',\n",
    "                  'Miles_driven_annually_bucket',\n",
    "                  'credit_history_bucket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has any NaN values as these values will make our algorithms throw an exception\n",
    "\n",
    "safe_driver.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Miles_driven_annually feature has some null values. Let us explore which particular cells have NaN and ingest them with the median data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_driver[safe_driver.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may make sense to ingest the median of  `Vehicle_Type=='Truck'` as all the NaN values are for Truck only. Let us look at the median of Miles_driven_annually by each vehicle type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values = safe_driver.groupby('Vehicle_Type').median()\n",
    "median_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in Miles_driven_annually with the median value for Truck\n",
    "# There may be better ways to impute missing data. But we have just 8 NaN cells out of some 30,000+ rows which is\n",
    "# less than 0.03%\n",
    "# So, imputing with median for all the 8 cells is not going to skew our results.\n",
    "\n",
    "safe_driver.fillna(\n",
    "    median_values.loc['Truck', 'Miles_driven_annually'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values again to make sure we did not miss any accidentally\n",
    "\n",
    "safe_driver[safe_driver.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of all remaining features\n",
    "\n",
    "safe_driver.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the feature values above, the range of values of each vary a lot. For example `'Miles_driven_annually'` is in the 10s of thousands, whereas 'credit_history' is in the 100s and 'annual-claims' is in single digit. Due to the varying magnitudes of the feature values we will scale the features with Z-scores using `sklearn.preprocessing.scale`.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To standardize the numeric features we need to isolate them first into a separate dataframe\n",
    "\n",
    "safe_driver_num_features = safe_driver.drop(\n",
    "    safe_driver.select_dtypes(['object']), axis=1)\n",
    "\n",
    "# Do not standardize 'target' which is our label\n",
    "\n",
    "safe_driver_num_features.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "safe_driver_cat_features = safe_driver.select_dtypes(['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any NaN values one more time\n",
    "\n",
    "safe_driver_num_features[safe_driver_num_features.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Restore the column names from the original dataset\n",
    "\n",
    "safe_driver_scaled = pd.DataFrame(preprocessing.scale(safe_driver_num_features),\n",
    "                                  columns=safe_driver_num_features.columns)\n",
    "\n",
    "# We now have the scaled feature set. Now we need to concatenate the categorical features back with our scaled\n",
    "# dataset before running OneHotEncoder or dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will concatenate the scaled dataframe with the categorical feature set\n",
    "\n",
    "safe_driver = pd.concat(\n",
    "    [safe_driver_scaled, safe_driver['target'], safe_driver_cat_features], axis=1)\n",
    "\n",
    "# We will add the 'target' label back to the scaled dataframe as we may need it later\n",
    "safe_driver_scaled = pd.concat(\n",
    "    [safe_driver_scaled, safe_driver['target']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_driver.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use data visualization techniques to find the distribution of the features and also the correlation between different features. We could, may be, drop one or two more features based on distribution or correlation making our dataset cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot matrix that shows all the bivariate relationships in one plot made up of subplots.\n",
    "# Let us drop the 'target' variable\n",
    "\n",
    "safe_driver_copy = safe_driver.drop(['target'], axis=1)\n",
    "\n",
    "# Plot with the remaining feature set\n",
    "\n",
    "g = sns.PairGrid(safe_driver_copy.dropna(),\n",
    "                 diag_sharey=False) #hue='Vehicle_Type')\n",
    "# As in the Unit 2 lesson example, create a Scatterplot in the top-right diagonal\n",
    "g.map_upper(plt.scatter, alpha=.5)\n",
    "# Linear relationship of two variables in the bottom-left diagonal\n",
    "g.map_lower(sns.regplot, scatter_kws=dict(alpha=0))\n",
    "# And...univariate distributions of the variables across the diagonal\n",
    "g.map_diag(sns.kdeplot, lw=3)\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The legend appears at the bottom-right plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to output a correlation heatmap that can tell us correlation coefficient of the features. If two variables are highly corrrelated our results could be incorrect or skewed.<br>\n",
    "<br>\n",
    "First we have to isolate the continuous variables in a dataframe before invoking the heatmap.<br>\n",
    "<br>\n",
    "Let us create the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "color_map = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(safe_driver_num_features.corr(), annot=True, cmap=color_map)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are not highly correlated with our target variable. We can keep the remaining features as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_driver_num_features = pd.concat(\n",
    "    [safe_driver_num_features, safe_driver['target']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_driver_num_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also look at the relationship between our dependent variable with categorical variables\n",
    "\n",
    "# Plot all the variables with boxplots for each continuous variable.\n",
    "\n",
    "# Restructure the data so we can use FacetGrid\n",
    "\n",
    "safe_driver_melt = pd.melt(safe_driver_scaled, id_vars=['target'])\n",
    "safe_driver_melt.info()\n",
    "g = sns.FacetGrid(safe_driver_melt, col='variable', size=4, aspect=.5)\n",
    "g = g.map(sns.boxplot, \"target\", \"value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our boxplots indicate that there are some outliers in EngineHP, credit_history and Miles_driven_annually. But we may need to keep the outliers unless they affect our results and take another look at them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, below, we separate our feature set from the label `target` and convert all the categorical variables to numeric. Then split the feature set into training and test data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert some of the categorical features into numeric giving weightage to each variable.<br>\n",
    "<br>\n",
    "1. Gender: 1 = Female and 2 = Male<br>\n",
    "2. Marital_Status: 1 = Single and 2 = Married<br>\n",
    "3. Vehicle_Type: Use `LabelEncoder`<br>\n",
    "4. Age_bucket: Use `LabelEncoder`<br>\n",
    "<br>\n",
    "We are not using `dummies` or `OneHotEncoder` because these create sparse matrices and increase dimensionality. By giving a 1 or a 2 for say Marital_Status we give higher weightage to `Married` by assigning a value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_driver.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Gender to a 1 or a 2\n",
    "safe_driver['Gender'] = np.where(safe_driver['Gender'] == 'F', 1, 2)\n",
    "\n",
    "# Convert Marital_Status to a 1 or a 2\n",
    "safe_driver['Marital_Status'] = np.where(\n",
    "    safe_driver['Marital_Status'] == 'Single', 1, 2)\n",
    "\n",
    "# Convert Vehicle_Type using LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(safe_driver['Vehicle_Type'])\n",
    "\n",
    "safe_driver['Vehicle_Type'] = le.transform(safe_driver['Vehicle_Type'])\n",
    "\n",
    "# Convert Age_bucket using LabelEncoder\n",
    "le.fit(safe_driver['Age_bucket'])\n",
    "\n",
    "safe_driver['Age_bucket'] = le.transform(safe_driver['Age_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_driver.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'target' column from training dataframe as that is our label\n",
    "X = safe_driver.drop(['target', 'State'], 1)\n",
    "\n",
    "# The 'target' column is our label or outcome that we want to predict\n",
    "y = safe_driver['target']\n",
    "\n",
    "# Use pd.dummies to resolve the categorical data (e.g. State) into numerical values\n",
    "#X = pd.get_dummies(X)\n",
    "\n",
    "# Drop and NaN values\n",
    "X = X.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out much earlier that our target label is 70% failure (bad driver or `target` == 1) and 30% success (good driver or `target` == 0). Let us do class balancing using SMOTE and see the distribution.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os = SMOTE(random_state=0)\n",
    "\n",
    "columns = X.columns\n",
    "os_data_X, os_data_y = os.fit_sample(X, y)\n",
    "os_data_X = pd.DataFrame(data=os_data_X, columns=columns)\n",
    "os_data_y = pd.DataFrame(data=os_data_y, columns=['y'])\n",
    "\n",
    "# Split the resulting balanced data set as train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    os_data_X, os_data_y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Check the size of our new data\n",
    "print(\"length of oversampled data is \", len(os_data_X))\n",
    "print(\"Number of negative class in oversampled data\",\n",
    "      len(os_data_y[os_data_y['y'] == 0]))\n",
    "print(\"Number of positive class in oversampled data\",\n",
    "      len(os_data_y[os_data_y['y'] == 1]))\n",
    "print(\"Proportion of negative class in oversampled data is \",\n",
    "      len(os_data_y[os_data_y['y'] == 0])/len(os_data_X))\n",
    "print(\"Proportion of positive class in oversampled data is \",\n",
    "      len(os_data_y[os_data_y['y'] == 1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find out how significant are our features are in predicting our label. We will use the feature_importances_ method from the RandomForestClassifier. After that we plot the relative importance of the features using a barplot.<br>\n",
    "<br>\n",
    "Let us go ahead and select our categorical features, using a RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the feature importance using RandomForest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=12)\n",
    "regr.fit(X, y)\n",
    "\n",
    "# The features identified by RandomForest will be our columns for the training and testing dataset\n",
    "\n",
    "feature_importances = pd.DataFrame(regr.feature_importances_, index=X.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = regr.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decision Tree Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LogisticRgression model on the training data set\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_predict = tree.predict(X_test)\n",
    "tree.score(X_test, y_test)\n",
    "\n",
    "dt_train_scores = cross_val_score(\n",
    "    estimator=tree, X=X_train, y=y_train, cv=5, n_jobs=4)\n",
    "dt_test_scores = cross_val_score(\n",
    "    estimator=tree, X=X_test, y=y_predict, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Accuracy, Error, Sensitivity and Specificity from the returned results\n",
    "target_names = ['Safe Driver', 'Non-safe Driver']\n",
    "decision_tree = classification_report(\n",
    "    y_test, y_predict, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for DecisionTreeClassifier\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = tree.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "\n",
    "# plot no skill\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curve')\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our confusion matrix based on the DecisionTree does not look good. It is showing a high number of\n",
    "false positives and false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Support Vector Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Support Vector Classifier to verify accuracy\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto', probability=True)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_predict = svc.predict(X_train)\n",
    "\n",
    "svc.score(X_test, y_test)\n",
    "\n",
    "svc_train_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
    "svc_test_score = cross_val_score(svc, X_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svc_train_score)\n",
    "print(svc_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = svc.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "\n",
    "# plot no skill\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curve')\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Safe Driver', 'Non-safe Driver']\n",
    "svc_scores = classification_report(\n",
    "    y_train, y_predict, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM Classifier returns less than better results than the DecisionTree model.<br>\n",
    "<br>\n",
    "We will try the SGDClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stochastic Gradient Descent Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also run the SGDClassifier model to verify\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(max_iter=10000, tol=1e-3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict_SGD = clf.predict(X_train)\n",
    "\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_train_score = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "sgd_test_score = cross_val_score(clf, X_test, y_test, cv=5)\n",
    "print(sgd_train_score)\n",
    "print(sgd_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Accuracy, Error, Sensitivity and Specificity from the returned results.\n",
    "target_names = ['Safe Driver', 'Non-safe Driver']\n",
    "sgd_scores = classification_report(\n",
    "    y_train, y_predict, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_predict_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Support Vector Classifier with different tuning parameters</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying SVC again with better tuning parameters found out from StackOverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "                 decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "                 max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                 tol=0.001, verbose=False)\n",
    "\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "y_predict = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_2_train_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "svc_2_test_score = cross_val_score(classifier, X_test, y_test, cv=5)\n",
    "print(svc_2_train_score)\n",
    "print(svc_2_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Accuracy, Error, Sensitivity and Specificity from the returned results.\n",
    "target_names = ['Safe Driver', 'Non-safe Driver']\n",
    "svc_2_scores = classification_report(\n",
    "    y_train, y_predict, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ridge Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us run RidgeClassifier to check results\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "#\n",
    "clf = RidgeClassifier().fit(X_train, y_train)\n",
    "clf.score(X_train, y_train) \n",
    "y_predict = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_score = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "ridge_test_score = cross_val_score(clf, X_test, y_test, cv=5)\n",
    "print(ridge_train_score)\n",
    "print(ridge_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Safe Driver', 'Non-safe Driver']\n",
    "ridge_scores = classification_report(y_train, y_predict, target_names=target_names,\n",
    "                                     output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient Boosting Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we try GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(loss='deviance', max_depth=10)\n",
    "clf_model = clf.fit(X_train, y_train)\n",
    "print(clf_model)\n",
    "print('Training set score:', clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF_score = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print('\\nEach Cross Validated Accuracy: \\n', CLF_score)\n",
    "print(\"\\nOverall Gradient Boosted Classifier Accuracy: %0.2f (+/- %0.2f)\\n\" %\n",
    "      (CLF_score.mean(), CLF_score.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF_test_score = cross_val_score(clf, X_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(X_train)\n",
    "target_names = ['Safe Driver', 'Non-safe Driver']\n",
    "GB_scores = classification_report(\n",
    "    y_train, y_predict, target_names=target_names, output_dict=True)\n",
    "confusion_matrix(y_train, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting seems to be experiencing overfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try XGBoost model which is supposed to be the 'Queen of All' models or 'GB on Steroids'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_predict = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_scores = classification_report(\n",
    "    y_test, y_predict, target_names=target_names, output_dict=True)\n",
    "XGB_train_score = cross_val_score(model, X_train, y_train, cv=5)\n",
    "XGB_test_score = cross_val_score(model, X_test, y_test, cv=5)\n",
    "print(XGB_train_score)\n",
    "print(XGB_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Summary of Precision and Recall scores</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the classification scores and plot results as a bargraph to compare performances\n",
    "# of different models\n",
    "\n",
    "# This plot is for Safe Driver class. Values plotted are the precision and recall scores\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "n_groups = 7\n",
    "\n",
    "precision = (decision_tree['Safe Driver']['precision'],\n",
    "             svc_scores['Safe Driver']['precision'],\n",
    "             sgd_scores['Safe Driver']['precision'],\n",
    "             svc_2_scores['Safe Driver']['precision'],\n",
    "             ridge_scores['Safe Driver']['precision'],\n",
    "             GB_scores['Safe Driver']['precision'],\n",
    "             XGB_scores['Safe Driver']['precision'])\n",
    "\n",
    "recall = (decision_tree['Safe Driver']['recall'],\n",
    "          svc_scores['Safe Driver']['recall'],\n",
    "          sgd_scores['Safe Driver']['recall'],\n",
    "          svc_2_scores['Safe Driver']['recall'],\n",
    "          ridge_scores['Safe Driver']['recall'],\n",
    "          GB_scores['Safe Driver']['recall'],\n",
    "          XGB_scores['Safe Driver']['recall'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = ax.bar(index, precision, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                error_kw=error_config,\n",
    "                label='Precision')\n",
    "\n",
    "rects2 = ax.bar(index + bar_width, recall, bar_width,\n",
    "                alpha=opacity, color='r',\n",
    "                error_kw=error_config,\n",
    "                label='Recall')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=10)\n",
    "ax.set_ylabel('Scores', fontsize=10)\n",
    "ax.set_title('Safe Driver scores by model and classification', fontsize=10)\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(('D-Tree', 'SVC', 'SGD', 'SVC-2',\n",
    "                    'Ridge', 'GB', 'XGB'), fontsize=10)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('Safe_Driver_Bargraph_200.eps', dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the classification scores and plot results as a bargraph to compare performances\n",
    "# of different models\n",
    "\n",
    "# This plot is for Non-Safe Driver class. Values plotted are the precision and recall scores\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "\n",
    "n_groups = 7\n",
    "\n",
    "precision = (decision_tree['Non-safe Driver']['precision'],\n",
    "             svc_scores['Non-safe Driver']['precision'],\n",
    "             sgd_scores['Non-safe Driver']['precision'],\n",
    "             svc_2_scores['Non-safe Driver']['precision'],\n",
    "             ridge_scores['Non-safe Driver']['precision'],\n",
    "             GB_scores['Non-safe Driver']['precision'],\n",
    "             XGB_scores['Non-safe Driver']['precision'])\n",
    "\n",
    "recall = (decision_tree['Non-safe Driver']['recall'],\n",
    "          svc_scores['Non-safe Driver']['recall'],\n",
    "          sgd_scores['Non-safe Driver']['recall'],\n",
    "          svc_2_scores['Non-safe Driver']['recall'],\n",
    "          ridge_scores['Non-safe Driver']['recall'],\n",
    "          GB_scores['Non-safe Driver']['recall'],\n",
    "          XGB_scores['Non-safe Driver']['recall'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = ax.bar(index, precision, bar_width,\n",
    "                alpha=opacity, color='g',\n",
    "                error_kw=error_config,\n",
    "                label='Precision')\n",
    "\n",
    "rects2 = ax.bar(index + bar_width, recall, bar_width,\n",
    "                alpha=opacity, color='m',\n",
    "                error_kw=error_config,\n",
    "                label='Recall')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Non-safe Driver scores by model and classification')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(('D-Tree', 'SVC', 'SGD', 'SVC-2', 'Ridge', 'GB', 'XGB'))\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('Non-safe_Driver_Bargraph_200.eps', dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation scores that can be plotted for visuals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "\n",
    "n_groups = 7\n",
    "\n",
    "cv_train = (dt_train_scores[0],\n",
    "            svc_train_score[0],\n",
    "            sgd_train_score[0],\n",
    "            svc_2_train_score[0],\n",
    "            ridge_train_score[0],\n",
    "            CLF_score[0],\n",
    "            XGB_train_score[0])\n",
    "\n",
    "cv_test = (dt_test_scores[0],\n",
    "           svc_test_score[0],\n",
    "           sgd_test_score[0],\n",
    "           svc_2_test_score[0],\n",
    "           ridge_test_score[0],\n",
    "           CLF_test_score[0],\n",
    "           XGB_test_score[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = ax.bar(index, cv_train, bar_width,\n",
    "                alpha=opacity, color='k',\n",
    "                error_kw=error_config,\n",
    "                label='CV on Training')\n",
    "\n",
    "rects2 = ax.bar(index + bar_width, cv_test, bar_width,\n",
    "                alpha=opacity, color='c',\n",
    "                error_kw=error_config,\n",
    "                label='CV on Test')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('CV Scores')\n",
    "ax.set_title('CV scores by model')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(('D-Tree', 'SVC', 'SGD', 'SVC-2', 'Ridge', 'GB', 'XGB'))\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('CV_Scores.eps', dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
